<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<HTML><HEAD><TITLE>CTO : Migration Comparison Part 1</TITLE
><LINK REL="alternate" TYPE="application/rss+xml" TITLE="Recent Changes" HREF="/tunes.org/cliki/recent-changes.rdf.html"><LINK REL="stylesheet" HREF="admin/cliki.css"></HEAD
><BODY><DIV ID="banner"><A TITLE="CTO" CLASS="logo" HREF="migration_20comparison_20part_201.html">CTO</A
> <SPAN>CLiki for the TUNES project</SPAN
></DIV
><DIV ID="navbar"><A CLASS="internal" HREF="index.html">Home</A
>  <A CLASS="internal" HREF="cliki.html">About CLiki</A
> <A CLASS="internal" HREF="text_20formatting.html">Text Formatting</A
> </DIV
><DIV ID="main"><H1>Migration Comparison Part 1</H1
>
The simplest traditional technique to break the limits of sequential computing is traditional symmetric multiprocessing: multiple high-end processors as a pool in a same (very expensive) computer are dispatched coarse-grained process to execute, the computing model for each process being the traditional sequential model. Actually, this expensive and technologically limited hardware technique is just an expedient hack to long the life of the traditional centralized "client/server" model that allows the server to manage more requests at once. But it doesn't allow to take advantage of the growing multi-computer networks it relies upon. It can be considered as the stubborn brute-force approach to the problem of parallel computing.

<p>Then comes traditional distributed computing, which is a software implementation of the previous approach, where coarse-grained processes are dispatched over a pool of logically equivalent processors in a computer network. Much better than the above method (that it encompasses, but for the ease of dynamical load-balancing), traditional distributed computing does allow to take advantage of existing and growing computer networks. With some refinement, and the choice of a compromise emulated virtual processor, it can even run on a heterogeneous network of computers.

<p>Traditional parallel computing rather relies on hardware clusters of similar processors to run a same program; thus, contrarily to traditional symmetric multiprocessing or distributed computing, the parallelism is now fine-grained. The problem with it is the high cost of parallel hardware, and its complete lack of adaptativity, as different problems each formalize into different network geometry, that a same parallel hardware cannot all provide at reasonable cost. Thus, unless a particular class of similar problems is in mind, specialized parallel hardware costs too much to be worth it, notwithstanding the evolution of computer hardware that will make such an expensive machine quickly obsolete.

<p>Of course, this traditional parallel computing can in turn be software emulated, as above, from a homogeneous or heterogeneous network of computers, but then, dispatching techniques that were successfully used for coarse-grained computing prove to have too large an overhead for fine-grained computing, all the more when the computer network is heterogeneous as of computer architecture, speed, transmission latency, reliability, security.

<p>If we restate this last problem as that of fine-grained distributed computing in a dynamically evolving network, we obtain migration as a possible solution. If we insist just a bit on the (not yet traditional) aspects of persistence and dynamism in such distributed computing, migration becomes a natural solution: computer objects are migrated in the network, so as to find a place where resources are enough, while respecting constraints of availability to the I/O nodes that request the object and unavailability to unsecure I/O nodes.

<p><a class="internal" href="migration_20comparison_20part_202.html" >Next</a><hr><p><b>This page is linked from: </b> <a class="internal" href="migration_20comparison.html" >Migration Comparison</a> &nbsp; <a class="internal" href="migration_20comparison_20intro.html" >Migration Comparison Intro</a> &nbsp; </DIV></BODY></HTML>